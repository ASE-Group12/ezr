<h1 id="easier-scripting">Easier Scripting</h1>
<p><em>Tim Menzies <a href="mailto:timm@ieee.org"
class="email">timm@ieee.org</a></em></p>
<p>The best way to write less code is to know more about coding.
Experienced developers can replace large and complicated code with
something much smaller and simpler things. They can do that since they
know the tips and tricks.</p>
<p>But where can you learn those tips and tricks? Well, let’s try to
learn them here. In this document we take something seemingly hard and
code it as simple as we can. Along the way, we offer lessons on the tips
and tricks used to write the smaller codebase.</p>
<h2 id="our-case-study-active-learning">Our Case Study: Active
Learning</h2>
<p>Suppose you are looking at a large number of things, and you do not
know which are any good. You could try them all, but that can take a lot
of time. In turns out this is a common problem:</p>
<ul>
<li>You like fishing but the ocean is a big place so in any day, you can
only try a few of your favorite spots.</li>
<li>You are a software engineer with:
<ul>
<li>too many tests to run;<br />
</li>
<li>too many Makefile options to try out;</li>
<li>a data miner with bewildering number of tuning parameters.</li>
</ul></li>
</ul>
<p>In all these cases, you know the space of possible choices, but you
do not have time to score each one. So the task is to score the
<em>fewest things</em> before finding <em>some things</em> that are
<em>good enough</em> (technically, this is an <em>active learning</em>
<a href="#fn1" class="footnote-ref" id="fnref1"
role="doc-noteref"><sup>1</sup></a> problem for <em>multi-objective
optimization</em> <a href="#fn2" class="footnote-ref" id="fnref2"
role="doc-noteref"><sup>2</sup></a> striving for <em>heaven</em> <a
href="#fn3" class="footnote-ref" id="fnref3"
role="doc-noteref"><sup>3</sup></a>). But what does <em>good</em> mean?
Well:</p>
<ul>
<li>One rule of thumb is that two things are indistinguishable<br />
if they differ by less than 35% of the standard deviation <a href="#fn4"
class="footnote-ref" id="fnref4"
role="doc-noteref"><sup>4</sup></a>.</li>
<li>Another rule of thumb is that, effectively, the standard deviation
ranges -3 to 3.</li>
<li>This means it is the region where we are indistinguishable from best
has size <span class="math inline">0.35/(3−−3) ≈ 6</span>% of all.</li>
</ul>
<p>How hard is it to find 6% of a set of solutions? According to
probable correctness theory <a href="#fn5" class="footnote-ref"
id="fnref5" role="doc-noteref"><sup>5</sup></a>, the certainty <span
class="math inline"><em>C</em></span> of observing an event with
probable <span class="math inline"><em>p</em></span> after <span
class="math inline"><em>n</em></span> random evaluations, is <span
class="math inline"><em>C</em> = 1 − (1−<em>p</em>)<sup><em>n</em></sup></span>.
This can be re-arranged to report the number of <span
class="math inline"><em>n</em> = <em>l</em><em>o</em><em>g</em>(1−<em>C</em>)/<em>l</em><em>o</em><em>g</em>(1−<em>p</em>)</span>.
So at confidence 0.95, we are need <span
class="math inline"><em>l</em><em>o</em><em>g</em>(1−.95)/<em>l</em><em>o</em><em>g</em>(1−0.06) ≈ 50</span>
random samples.</p>
<p>And we might be able to do better than that. Suppose we have some way
to peek at all our examples and guess which are better than the rest.
Given a good “peeker”, we could, in theory, search our examples via some
binary search procedure, using <span
class="math inline"><em>l</em><em>o</em><em>g</em><sub>2</sub>(50) = 6</span>
comparisons <a href="#fn6" class="footnote-ref" id="fnref6"
role="doc-noteref"><sup>6</sup></a>.</p>
<p>This lower value</p>
<p>means that tIf we only want to find something like the best thing,
then we only need to get there will be a space of things near the best
we don’t want to find the best thing, butn</p>
<p>== Main</p>
<h2 id="ez2.py-types">&lt;ez2.py types&gt;</h2>
<p>&lt;1&gt; asdsadasasa asdasd asdasddasas &lt;2&gt; asdasdsdasds</p>
<p>== Conclusion</p>
<p>That’s all, folks!</p>
<aside id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p><em>Active learning</em> means building a predictive
modeling while using as few labels as possible. Active learners guess
labels for examples one at a time; adjust their models depending on how
good was their last guess; then suggest which example to look at next.<a
href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p><em>Multi-objective optimizers</em> try to find
combinations of model inputs that lead to best goal values. When
multiple goals contradict each other, it may be required to trade-off
one goal for another.<a href="#fnref2" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>We say that our optimizers are striving to reach some
<em>heaven</em> point. For example, say our cars use between 10 to 30
miles per gallon and accelerate to 60mph in in 5 to 20 seconds. If we
want fast acceleration and good miles per hour, heaven is the point
(10,5) for (mph,acceleration).<a href="#fnref3" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Standard deviation, or <span
class="math inline"><em>σ</em></span>, is a measure of much a set of
numbers wobble around their middle value. If you sort a 100 numbers in a
list <span class="math inline"><em>a</em></span> then <span
class="math inline"><em>σ</em> = (<em>a</em>[90]=<em>a</em>[10])/2.56</span>.
Also if you read those numbers, one at a time you can work <span
class="math inline"><em>σ</em></span> as follows. Initialize
<em>n=0,<span class="math inline"><em>μ</em></span>=0,m2=0</em> then for
each new number <span class="math inline"><em>x</em></span> then (a)
<span class="math inline"><em>n</em> = <em>n</em> + 1</span>; (b) <span
class="math inline"><em>d</em> = <em>x</em> − <em>μ</em></span>; (c)
<span
class="math inline"><em>μ</em> = <em>μ</em> + <em>d</em>/<em>n</em></span>;
(d) <span
class="math inline"><em>m</em>2 = <em>m</em>2 + <em>d</em> * (<em>x</em>−<em>μ</em>)</span>;
(e)<span
class="math inline"><em>σ</em> = (<em>m</em>2/(<em>n</em>−1))<sup>0.5</sup></span>
.<a href="#fnref4" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Hamlet, Richard G. “Probable correctness theory.”
Information processing letters 25.1 (1987): 17-25. If something happens
at probability <span class="math inline"><em>p</em></span>, it does not
happen after <span class="math inline"><em>n</em></span> trials at
probability <span
class="math inline">(1−<em>p</em>)<sup><em>n</em></sup></span>. Which
means it happens at certainty <span
class="math inline"><em>C</em> = 1 − (1−<em>p</em>)<sup><em>n</em></sup></span>.<a
href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>This calculation makes certain optimistic assumption
that may not hold in reality. For example, it assumes that our models
are simple Gaussians and that all solutions are equally spaced across
the x-axis. Nevertheless, as we shall see, ass<a href="#fnref6"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
